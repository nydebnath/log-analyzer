{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG-based AI Log Analysis with LangChain\n",
    "\n",
    "Overview\n",
    "\n",
    "This document outlines the implementation of a Retrieval-Augmented Generation (RAG)-based AI system for log analysis. The system utilizes LangChain, FAISS, and OpenAI to enable intelligent querying of log files, providing contextual and insightful responses to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation Steps\n",
    "\n",
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import faiss\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Log Monitor Module\n",
    "\n",
    "This module uses the watchdog library to monitor log file updates and trigger processing for new log entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogMonitor(FileSystemEventHandler):\n",
    "    def __init__(self, log_file_path, on_update_callback):\n",
    "        self.log_file_path = log_file_path\n",
    "        self.on_update_callback = on_update_callback\n",
    "\n",
    "    def on_modified(self, event):\n",
    "        if event.src_path == self.log_file_path:\n",
    "            with open(self.log_file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            self.on_update_callback(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessor Module\n",
    "\n",
    "This module prepares raw log entries for vectorization by performing basic text cleaning and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logs(log_lines):\n",
    "    processed_logs = []\n",
    "    for line in log_lines:\n",
    "        processed_logs.append(line.strip().lower())\n",
    "    return processed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorizer and FAISS Integration with LangChain\n",
    "\n",
    "The integration of FAISS and LangChain enables efficient storage and retrieval of log entries based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainFAISSHandler:\n",
    "    def __init__(self):\n",
    "        self.embedding = OpenAIEmbeddings()\n",
    "        self.vector_store = FAISS(embedding_function=self.embedding)\n",
    "        self.system_context = \"You are a helpful and intelligent log analyzer. Answer user queries based on the provided log data.\"\n",
    "\n",
    "    def add_logs(self, logs):\n",
    "        for log in logs:\n",
    "            self.vector_store.add_texts([log])\n",
    "\n",
    "    def query_logs(self, message, history):\n",
    "        relevant_system_context = self.system_context\n",
    "        messages = (\n",
    "            [{\"role\": \"system\", \"content\": relevant_system_context}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "        )\n",
    "\n",
    "        qa_chain = VectorDBQA(llm=OpenAI(), vectorstore=self.vector_store)\n",
    "        response = qa_chain.run(messages)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration Workflow\n",
    "\n",
    "This workflow ties together log monitoring, preprocessing, vectorization, and querying into a seamless pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_logs_and_query(log_file_path, message, history):\n",
    "    # Step 1: Monitor Logs\n",
    "    if not hasattr(process_logs_and_query, \"faiss_handler\"):\n",
    "        process_logs_and_query.faiss_handler = LangChainFAISSHandler()\n",
    "\n",
    "    def on_log_update(new_logs):\n",
    "        processed_logs = preprocess_logs(new_logs)\n",
    "        process_logs_and_query.faiss_handler.add_logs(processed_logs)\n",
    "\n",
    "    log_monitor = LogMonitor(log_file_path, on_log_update)\n",
    "    observer = Observer()\n",
    "    observer.schedule(log_monitor, path=os.path.dirname(log_file_path), recursive=False)\n",
    "    observer.start()\n",
    "\n",
    "    # Step 2: Query Logs\n",
    "    return process_logs_and_query.faiss_handler.query_logs(message, history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Entry Point for Chat Integration\n",
    "\n",
    "To enable user interaction through a UI like Gradio or other communication channels (e.g., Webex bot), the following entry point is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    log_file_path = '/path/to/logfile.log'  # Path to your log file\n",
    "    history = history or []\n",
    "    response = process_logs_and_query(log_file_path, message, history)\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "chat_ui = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    type=\"messages\"\n",
    ")\n",
    "chat_ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG-based AI Log Analysis with LangChain\n",
    "\n",
    "Overview\n",
    "\n",
    "This document outlines the implementation of a Retrieval-Augmented Generation (RAG)-based AI system for log analysis. The system utilizes LangChain, FAISS, and OpenAI to enable intelligent querying of log files, providing contextual and insightful responses to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation Steps\n",
    "\n",
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import faiss\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Log Monitor Module\n",
    "\n",
    "This module uses the watchdog library to monitor log file updates and trigger processing for new log entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogMonitor(FileSystemEventHandler):\n",
    "    def __init__(self, log_file_path, on_update_callback):\n",
    "        self.log_file_path = log_file_path\n",
    "        self.on_update_callback = on_update_callback\n",
    "\n",
    "    def on_modified(self, event):\n",
    "        if event.src_path == self.log_file_path:\n",
    "            with open(self.log_file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            self.on_update_callback(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessor Module\n",
    "\n",
    "This module prepares raw log entries for vectorization by performing basic text cleaning and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logs(log_lines):\n",
    "    processed_logs = []\n",
    "    for line in log_lines:\n",
    "        processed_logs.append(line.strip().lower())\n",
    "    return processed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorizer and FAISS Integration with LangChain\n",
    "\n",
    "The integration of FAISS and LangChain enables efficient storage and retrieval of log entries based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainFAISSHandler:\n",
    "    def __init__(self):\n",
    "        self.embedding = OpenAIEmbeddings()\n",
    "        self.vector_store = FAISS(embedding_function=self.embedding)\n",
    "        self.system_context = \"You are a helpful and intelligent log analyzer. Answer user queries based on the provided log data.\"\n",
    "\n",
    "    def add_logs(self, logs):\n",
    "        for log in logs:\n",
    "            self.vector_store.add_texts([log])\n",
    "\n",
    "    def query_logs(self, query):\n",
    "        qa_chain = VectorDBQA(\n",
    "            llm=OpenAI(), \n",
    "            vectorstore=self.vector_store\n",
    "        )\n",
    "        full_prompt = [\n",
    "            {\"role\": \"system\", \"content\": self.system_context},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "        response = qa_chain.run(full_prompt)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration Workflow\n",
    "\n",
    "This workflow ties together log monitoring, preprocessing, vectorization, and querying into a seamless pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_logs_and_query(log_file_path, query):\n",
    "    # Step 1: Monitor Logs\n",
    "    def on_log_update(new_logs):\n",
    "        processed_logs = preprocess_logs(new_logs)\n",
    "        faiss_handler.add_logs(processed_logs)\n",
    "\n",
    "    log_monitor = LogMonitor(log_file_path, on_log_update)\n",
    "    observer = Observer()\n",
    "    observer.schedule(log_monitor, path=os.path.dirname(log_file_path), recursive=False)\n",
    "    observer.start()\n",
    "\n",
    "    # Step 2: Query Logs\n",
    "    return faiss_handler.query_logs(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "log_file_path = '/path/to/logfile.log'\n",
    "query = \"Why are there repeated login failures?\"\n",
    "result = process_logs_and_query(log_file_path, query)\n",
    "print(\"Query Result:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
